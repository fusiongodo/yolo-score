{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1436aee-0484-4959-ae54-24d9046f2e9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to: /workspace/mydir/obb_anns_hausarbeit/ds2_dense/ds2_dense/gt_space.json\n",
      "[loadMode] Loaded weights from model_dumps/wtf_is_goingon.pth\n",
      "2000 crops processed\n",
      "0.03234643167771509\n",
      "2000 crops processed\n",
      "0.04058524032746028\n",
      "2000 crops processed\n",
      "0.04877234794370207\n",
      "2000 crops processed\n",
      "0.03369824144624282\n",
      "2000 crops processed\n",
      "0.03707673218587054\n",
      "2000 crops processed\n",
      "0.04083274530393496\n",
      "Epoch 1 mAP:3.459798e-02total:95.6799 l_xy:9.8120 l_wh:42.8227 l_obj:1.9635 l_noobj:3.7060 l_cls:37.3758\n",
      "2000 crops processed\n",
      "0.045448861611804\n",
      "2000 crops processed\n",
      "0.061889548869882134\n",
      "2000 crops processed\n",
      "0.0431012127259363\n",
      "2000 crops processed\n",
      "0.05231140597291656\n",
      "2000 crops processed\n",
      "0.03612049669914694\n",
      "2000 crops processed\n",
      "0.0524152043816045\n",
      "Epoch 2 mAP:3.165992e-02total:80.5123 l_xy:9.2501 l_wh:34.4128 l_obj:1.7968 l_noobj:3.3983 l_cls:31.6542\n",
      "2000 crops processed\n",
      "0.05506694767998717\n",
      "2000 crops processed\n",
      "0.05277196474392525\n",
      "2000 crops processed\n",
      "0.06602160499931543\n",
      "2000 crops processed\n",
      "0.06106501141261063\n",
      "2000 crops processed\n",
      "0.0742024108756037\n",
      "2000 crops processed\n",
      "0.04302932144932726\n",
      "Epoch 3 mAP:3.274516e-02total:70.0163 l_xy:8.7571 l_wh:29.4310 l_obj:1.6367 l_noobj:3.1339 l_cls:27.0577\n",
      "2000 crops processed\n",
      "0.0715673603830568\n",
      "2000 crops processed\n",
      "0.05356296516636412\n",
      "2000 crops processed\n",
      "0.045899912231686955\n",
      "2000 crops processed\n",
      "0.0819185343863171\n",
      "2000 crops processed\n",
      "0.06513585977609679\n",
      "2000 crops processed\n",
      "0.049696349839356696\n",
      "Epoch 4 mAP:4.924549e-02total:60.8667 l_xy:8.2919 l_wh:25.3497 l_obj:1.4663 l_noobj:2.8969 l_cls:22.8620\n",
      "[saveModel] Saved weights to model_dumps/wtf_is_goingon_ep4_28071859.pth (device=cuda:0)\n",
      "[Trainer] Saved checkpoint wtf_is_goingon_ep4_28071859.pth\n",
      "2000 crops processed\n",
      "0.050747882045955775\n",
      "2000 crops processed\n",
      "0.09539092136251029\n",
      "2000 crops processed\n",
      "0.08111725813719062\n",
      "2000 crops processed\n",
      "0.06079645229399693\n",
      "2000 crops processed\n",
      "0.09110095928727886\n",
      "Epoch 5 mAP:5.515732e-02total:52.6699 l_xy:7.8414 l_wh:21.8133 l_obj:1.3330 l_noobj:2.6694 l_cls:19.0128\n",
      "2000 crops processed\n",
      "0.07457949318419707\n",
      "2000 crops processed\n",
      "0.0861645668383953\n",
      "2000 crops processed\n",
      "0.12308522602977007\n",
      "2000 crops processed\n",
      "0.09744842993018893\n",
      "2000 crops processed\n",
      "0.06452385668372969\n",
      "2000 crops processed\n",
      "0.09961214256349407\n",
      "Epoch 6 mAP:7.263829e-02total:46.4421 l_xy:7.4109 l_wh:19.4837 l_obj:1.3415 l_noobj:2.5743 l_cls:15.6317\n",
      "2000 crops processed\n",
      "0.0960659811571091\n",
      "2000 crops processed\n",
      "0.0825606883276106\n",
      "2000 crops processed\n",
      "0.13084267138311445\n",
      "2000 crops processed\n",
      "0.0827348421334473\n",
      "2000 crops processed\n",
      "0.12055601240710026\n",
      "2000 crops processed\n",
      "0.11586040410285477\n",
      "Epoch 7 mAP:1.027921e-01total:41.3875 l_xy:7.0077 l_wh:17.9221 l_obj:1.2948 l_noobj:2.4558 l_cls:12.7071\n",
      "2000 crops processed\n",
      "0.14766590520326958\n",
      "2000 crops processed\n",
      "0.13375791917685959\n",
      "2000 crops processed\n",
      "0.14279639978610276\n",
      "2000 crops processed\n",
      "0.0990347696613449\n",
      "2000 crops processed\n",
      "0.10827158590955868\n",
      "2000 crops processed\n",
      "0.12041507744822255\n",
      "Epoch 8 mAP:1.105357e-01total:36.8399 l_xy:6.6291 l_wh:16.5640 l_obj:1.1139 l_noobj:2.2619 l_cls:10.2709\n",
      "[saveModel] Saved weights to model_dumps/wtf_is_goingon_ep8_28071911.pth (device=cuda:0)\n",
      "[Trainer] Saved checkpoint wtf_is_goingon_ep8_28071911.pth\n",
      "2000 crops processed\n",
      "0.14254871750499162\n",
      "2000 crops processed\n",
      "0.1305822576193924\n",
      "2000 crops processed\n",
      "0.1133082480520649\n",
      "2000 crops processed\n",
      "0.16334338372985513\n",
      "2000 crops processed\n",
      "0.13751993039268962\n",
      "Epoch 9 mAP:1.030579e-01total:31.9752 l_xy:6.2264 l_wh:14.7306 l_obj:1.0293 l_noobj:2.0406 l_cls:7.9483\n",
      "2000 crops processed\n",
      "0.18139494330590344\n",
      "2000 crops processed\n",
      "0.13261998957115154\n",
      "2000 crops processed\n",
      "0.16126967294260652\n",
      "2000 crops processed\n",
      "0.222320153776444\n",
      "2000 crops processed\n",
      "0.24901299240390273\n",
      "2000 crops processed\n",
      "0.19097324495803505\n",
      "Epoch 10 mAP:1.167266e-01total:28.5359 l_xy:5.8744 l_wh:13.4288 l_obj:0.9674 l_noobj:1.9583 l_cls:6.3070\n",
      "2000 crops processed\n",
      "0.1092658099512758\n",
      "2000 crops processed\n",
      "0.17328365786781802\n",
      "2000 crops processed\n",
      "0.18595629920252038\n",
      "2000 crops processed\n",
      "0.2135231062842483\n",
      "2000 crops processed\n",
      "0.14492289270817296\n",
      "2000 crops processed\n",
      "0.21050762849408244\n",
      "Epoch 11 mAP:1.830396e-01total:26.2908 l_xy:5.5483 l_wh:12.7073 l_obj:1.0163 l_noobj:1.8669 l_cls:5.1520\n",
      "2000 crops processed\n",
      "0.19964632454928527\n",
      "2000 crops processed\n",
      "0.19456107756044838\n",
      "2000 crops processed\n",
      "0.1932881899566407\n",
      "2000 crops processed\n",
      "0.19740143104518793\n",
      "2000 crops processed\n",
      "0.20267839864869372\n",
      "2000 crops processed\n",
      "0.1253776131934208\n",
      "Epoch 12 mAP:1.880029e-01total:23.3844 l_xy:5.1880 l_wh:11.5715 l_obj:0.9795 l_noobj:1.7469 l_cls:3.8985\n",
      "[saveModel] Saved weights to model_dumps/wtf_is_goingon_ep12_28071922.pth (device=cuda:0)\n",
      "[Trainer] Saved checkpoint wtf_is_goingon_ep12_28071922.pth\n",
      "[saveModel] Saved weights to model_dumps/wtf_is_goingon.pth (device=cuda:0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import dataset as d\n",
    "import model as m\n",
    "import trainer as t\n",
    "import util \n",
    "import eval as e\n",
    "import loss as l\n",
    "import config \n",
    "from importlib import reload\n",
    "\n",
    "\n",
    "reload(config)\n",
    "reload(m)\n",
    "reload(d)\n",
    "reload(l)\n",
    "reload(util)\n",
    "reload(t)\n",
    "reload(e)\n",
    "\n",
    "de = util.DataExtractor()\n",
    "gt_df = de.croppedData()\n",
    "\n",
    "\n",
    "dataset = d.CroppedDataset(gt_df, mode = \"train\")\n",
    "\n",
    "model = m.YOLOResNet()\n",
    "name = \"wtf_is_goingon\"\n",
    "model = util.loadModel(name, model) #0.0122, 0.0059   #concorde0->concorde00\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=16, num_workers=12)\n",
    "loss_fn = l.YOLOv2Loss(l_noobj = 0.10, l_wh = 1.0, l_obj = 4.0)\n",
    "trainer = t.Trainer(model, loss_fn, gt_df, loader, epochs=12,  lr=1e-4, modelname=name, save_interval=4, device='cuda')\n",
    "\n",
    "try:\n",
    "    trainer.run()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted. Saving model...\")\n",
    "\n",
    "util.saveModel(name, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cb6a460-b912-4c27-ba90-99d4ebe3eff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelSeries: loadJsonData()\n",
      "[loadMode] Loaded weights from /workspace/mydir/obb_anns_hausarbeit/models/train_with_lr1e-2/checkpoints/0.pth\n",
      "Saving to: /workspace/mydir/obb_anns_hausarbeit/ds2_dense/ds2_dense/gt_space.json\n",
      "dataset.py: All images loaded into RAM within 199.9881 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m modelseries = ms.ModelSeries(\u001b[33m\"\u001b[39m\u001b[33mtrain_with_lr1e-2\u001b[39m\u001b[33m\"\u001b[39m, model_descr=\u001b[33m\"\u001b[39m\u001b[33mYoloResNet@120x40\u001b[39m\u001b[33m\"\u001b[39m, mode = \u001b[33m\"\u001b[39m\u001b[33mtraining\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m lc = ms.LearnConfig(c_lr=\u001b[32m1e-2\u001b[39m, c_obj = \u001b[32m5.0\u001b[39m, c_noobj=\u001b[32m0.1\u001b[39m, c_xy = \u001b[32m1.0\u001b[39m, c_wh = \u001b[32m1.0\u001b[39m,  c_cls=\u001b[32m1.0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m trainer = \u001b[43mt2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelseries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_config\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_rate\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     19\u001b[39m     trainer.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/mydir/obb_anns_hausarbeit/trainer2.py:45\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, modelseries, learn_config, epochs, checkpoint_rate, num_workers, batch_size)\u001b[39m\n\u001b[32m     42\u001b[39m gt_df = de.croppedData()\n\u001b[32m     44\u001b[39m \u001b[38;5;28mself\u001b[39m.eval_dataset = CroppedDataset(gt_df, mode = \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28mself\u001b[39m.train_dataset = \u001b[43mCroppedDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mself\u001b[39m.eval_loader  = DataLoader(\u001b[38;5;28mself\u001b[39m.eval_dataset, batch_size=\u001b[38;5;28mself\u001b[39m.batch_size,\n\u001b[32m     47\u001b[39m                                shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers=num_workers, pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     48\u001b[39m \u001b[38;5;28mself\u001b[39m.train_loader = DataLoader(\u001b[38;5;28mself\u001b[39m.train_dataset, batch_size = \u001b[38;5;28mself\u001b[39m.batch_size, \n\u001b[32m     49\u001b[39m                                shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m,  num_workers=num_workers, )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/mydir/obb_anns_hausarbeit/dataset.py:44\u001b[39m, in \u001b[36mCroppedDataset.__init__\u001b[39m\u001b[34m(self, gt_df, mode)\u001b[39m\n\u001b[32m     42\u001b[39m counter = \u001b[32m0\u001b[39m\n\u001b[32m     43\u001b[39m start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[32m     45\u001b[39m     counter += \u001b[32m1\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m counter % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/mydir/obb_anns_hausarbeit/dataset.py:62\u001b[39m, in \u001b[36m_load_item\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/mydir/obb_anns_hausarbeit/util.py:494\u001b[39m, in \u001b[36mload_crop_image\u001b[39m\u001b[34m(img_path, crop_row, crop_col)\u001b[39m\n\u001b[32m    491\u001b[39m lower_px = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m((crop_row + \u001b[32m1\u001b[39m) * crop_px))\n\u001b[32m    492\u001b[39m scale = config.RES / (right_px - left_px)\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m full_img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRGB\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.resize((effective_full_size, effective_full_size))\n\u001b[32m    495\u001b[39m crop_img = full_img.crop((left_px, top_px, right_px, lower_px)).resize((config.RES, config.RES))\n\u001b[32m    497\u001b[39m crop_img = crop_img.convert(\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m) \n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py:993\u001b[39m, in \u001b[36mImage.convert\u001b[39m\u001b[34m(self, mode, matrix, dither, palette, colors)\u001b[39m\n\u001b[32m    990\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mBGR;15\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;16\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;24\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    991\u001b[39m     deprecate(mode, \u001b[32m12\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m993\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    995\u001b[39m has_transparency = \u001b[33m\"\u001b[39m\u001b[33mtransparency\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    997\u001b[39m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py:300\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    297\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[32m    299\u001b[39m b = b + s\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m n, err_code = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m0\u001b[39m:\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import trainer2 as t2\n",
    "import ModelSeries as ms\n",
    "import loss as l\n",
    "import util\n",
    "import model as m\n",
    "import dataset\n",
    "from importlib import reload\n",
    "reload(dataset)\n",
    "reload(t2)\n",
    "reload(l)\n",
    "reload(ms)\n",
    "reload(util)\n",
    "\n",
    "modelseries = ms.ModelSeries(\"train_with_lr1e-2\", model_descr=\"YoloResNet@120x40\", mode = \"training\")\n",
    "lc = ms.LearnConfig(c_lr=1e-2, c_obj = 5.0, c_noobj=0.1, c_xy = 1.0, c_wh = 1.0,  c_cls=1.0)\n",
    "\n",
    "trainer = t2.Trainer(modelseries, learn_config = lc, epochs = 10, checkpoint_rate = 5, num_workers = 10, batch_size = 16)\n",
    "try:\n",
    "    trainer.run()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted. Saving model...\")\n",
    "    print(\"training.ipynb - records.head()\", trainer.modelseries.records.head())\n",
    "    trainer.keyboardInterrupt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1cd45-bc52-47f8-87da-f0eee3c7ae56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
