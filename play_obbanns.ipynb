{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b2772e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "from torchvision.transforms.functional import crop, resize, rgb_to_grayscale\n",
    "from torch.utils.data import IterableDataset\n",
    "import itertools\n",
    "from obb_anns import OBBAnns\n",
    "from pathlib import Path\n",
    "import json\n",
    "filepath = r\"C:\\Users\\alexh\\Desktop\\ComputerVision\\Hausarbeit\\deepscores_v2_dense\\ds2_dense\\deepscores_train.json\"\n",
    "basePath = r\"C:\\Users\\alexh\\Desktop\\ComputerVision\\Hausarbeit\\deepscores_v2_dense\\ds2_dense\\images\\\\\" \n",
    "\n",
    "\n",
    "def to_grayscale(tensor):\n",
    "    # tensor: [C, H, W] mit C=3 → gibt [1, H, W] zurück\n",
    "    return rgb_to_grayscale(tensor, num_output_channels=1)\n",
    "\n",
    "class SymbolCropStreamer(IterableDataset):\n",
    "    def __init__(self,target_size=(64, 64), transform=None):\n",
    "        self.data = []\n",
    "        with open(filepath, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "            \n",
    "        self.o = OBBAnns(filepath)\n",
    "        self.o.load_annotations()\n",
    "        self.img_index = 0\n",
    "        self.ann_index = 0\n",
    "        self.img , self.current_anns = self.o.get_img_ann_pair([0])\n",
    "        self.current_src_img = None\n",
    "        self.load_first()\n",
    "        self.target_size = target_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def load_first(self):\n",
    "        self.img, self.current_anns = self.o.get_img_ann_pair([self.img_index])\n",
    "        img_path = Path(basePath) / self.img[0][\"filename\"]\n",
    "        self.current_src_img = read_image(img_path).float() / 255\n",
    "\n",
    "    def load_next(self):\n",
    "        self.img_index += 1\n",
    "        if self.img_index >= len(self.o):\n",
    "            return False\n",
    "        self.ann_index = 0\n",
    "        self.img, self.current_anns = self.o.get_img_ann_pair([self.img_index])\n",
    "        img_path = Path(basePath) / self.img[0][\"filename\"]\n",
    "        self.current_src_img = read_image(img_path).float() / 255\n",
    "        return True\n",
    "    \n",
    "    def get_bounding_box_cat(self):\n",
    "        bb, cat = self.current_anns[0][\"a_bbox\"].values[self.ann_index], self.current_anns[0][\"cat_id\"].values[self.ann_index]\n",
    "        if cat[0] == 135:\n",
    "            print(\"staff line\")\n",
    "            self.ann_index +=1\n",
    "            if self.ann_index >= len(self.current_anns[0]):\n",
    "                r = self.load_next()\n",
    "                if not r:\n",
    "                    return False\n",
    "            return self.get_bounding_box_cat()\n",
    "        return bb, cat[0]\n",
    "        \n",
    "\n",
    "    def __iter__(self):\n",
    "        while self.img_index < len(self.o):\n",
    "            if self.ann_index >= len(self.current_anns[0]):\n",
    "                r = self.load_next()\n",
    "                if not r:\n",
    "                    break\n",
    "            bounding_box, cat = self.get_bounding_box_cat()#Rekursion einführen\n",
    "            x, y, x1, y1 = map(int, bounding_box)\n",
    "            patch = crop(self.current_src_img, top=y, left=x, height=y1 - y, width=x1 - x)\n",
    "            patch = resize(patch, self.target_size)\n",
    "            if self.transform:\n",
    "                patch = self.transform(patch)\n",
    "            self.ann_index += 1\n",
    "            yield patch, cat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2393e85e",
   "metadata": {},
   "source": [
    "Todo\n",
    " - categories.json besser nachvollziehen. Wie würde der Datensatz aussehen, wenn wir nur mit deepscores arbeiten würden?\n",
    " - Model Trainer implementieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "285655cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_img(patch, cat):\n",
    "    print(f\"Patch shape:\", patch.shape)  # z.B. torch.Size([1, 28, 28])\n",
    "\n",
    "    if patch.dim() == 3 and patch.shape[0] == 1:\n",
    "        # [1, H, W] → [H, W]\n",
    "        img = patch.squeeze(0).cpu().numpy()\n",
    "        cmap = 'gray'\n",
    "    elif patch.dim() == 3 and patch.shape[0] == 3:\n",
    "        # [3, H, W] → [H, W, 3]\n",
    "        img = patch.permute(1, 2, 0).cpu().numpy()\n",
    "        cmap = None\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unerwartete Tensor‐Shape: {patch.shape}\")\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(img, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(f\"Category {cat}\", fontsize=10)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "efb8fc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ann_info...\n",
      "done! t=17.85s\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "# dataloader = SymbolCropStreamer()\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def createDataloader():\n",
    "    dataset = SymbolCropStreamer(target_size=(64, 64), transform=to_grayscale)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,        # z.B. 32 Patches pro Batch\n",
    "        num_workers=0,        # Anzahl Worker‐Prozesse (je nach CPU-Kernen anpassen)\n",
    "        pin_memory=True       # bei GPU-Training oft sinnvoll\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "dataloader = createDataloader()\n",
    "\n",
    "print(type(dataloader))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "416f239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n",
      "staff line\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "for n, (batch_patches, batch_cats) in  enumerate(dataloader):\n",
    "    l.append(batch_cats.tolist())\n",
    "    if(n >= 100):\n",
    "        \n",
    "        break\n",
    "    \n",
    "    for i in range(len(batch_cats)):\n",
    "       # print(batch_cats)\n",
    "       pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b93a3f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n"
     ]
    }
   ],
   "source": [
    "flat = [elem for sub in l for elem in sub]\n",
    "len(flat)\n",
    "print(max(flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "592ac761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc6fd8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ann_info...\n",
      "done! t=7.76s\n"
     ]
    }
   ],
   "source": [
    "filepath = r\"C:\\Users\\alexh\\Desktop\\ComputerVision\\Hausarbeit\\deepscores_v2_dense\\ds2_dense\\deepscores_train.json\"\n",
    "basePath = r\"C:\\Users\\alexh\\Desktop\\ComputerVision\\Hausarbeit\\deepscores_v2_dense\\ds2_dense\\images\\\\\" \n",
    "o = OBBAnns(filepath)\n",
    "o.load_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3d36c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"categories_sorted.json\", orient=\"index\", indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c9c54f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "class MyLeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyLeNet, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels=8, kernel_size = 5, stride = 1, padding = 0)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 8, out_channels=16, kernel_size = 5, stride = 1, padding = 0) \n",
    "        self.linear1 = nn.Linear(in_features = 2704, out_features = 120)\n",
    "        self.linear2 = nn.Linear(in_features = 120, out_features = 84)\n",
    "        self.linear3 = nn.Linear(in_features = 84, out_features = 136)\n",
    "\n",
    "    def forward(self, x):  \n",
    "        #32x64x64x1\n",
    "        x = self.relu(self.conv1(x)) \n",
    "        #32x60x60x8\n",
    "        x = self.pool(x) \n",
    "        #32x30x30x8\n",
    "        x = self.relu(self.conv2(x)) \n",
    "        #32x26x26x16\n",
    "        x = self.pool(x)\n",
    "        #32x13x13x16\n",
    "        x = self.relu(self.linear1(x.reshape(x.shape[0], -1)))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9429375d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897c6ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "687de693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "def train(model, dataloader, n_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # lr is the default/start value of the learning_rate that adam adjust for each parameter\n",
    "    total_step = len(dataloader)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%d.%m.%y_%H-%M-%S\")\n",
    "    log_file = open(f\"./training_logs/rgb_training_{timestamp}.txt\", \"w\") \n",
    "\n",
    "    losses = []\n",
    "\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, (images, labels) in enumerate(dataloader):  \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "                \n",
    "            #Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            avg_batch_loss = loss(outputs, labels)\n",
    "            losses.append(avg_batch_loss)\n",
    "            \n",
    "            #Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            avg_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i+68) % 68== 0:\n",
    "                log_file.write('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}\\n'.format(epoch+1, 10, i+1, total_step, avg_batch_loss.item()))\n",
    "                log_file.write(f\"{outputs}\\n\")\n",
    "                log_file.write(f\"{labels}\\n\")\n",
    "    log_file.close()\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fd674e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ann_info...\n",
      "done! t=5.84s\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'SymbolCropStreamer' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[98]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m dataloader1 = createDataloader()\n\u001b[32m      2\u001b[39m model1 = MyLeNet()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[97]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, dataloader, n_epochs)\u001b[39m\n\u001b[32m      9\u001b[39m loss = nn.CrossEntropyLoss()\n\u001b[32m     10\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001b[32m0.001\u001b[39m) \u001b[38;5;66;03m# lr is the default/start value of the learning_rate that adam adjust for each parameter\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m total_step = \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m timestamp = datetime.now().strftime(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm.\u001b[39m\u001b[33m%\u001b[39m\u001b[33my_\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m log_file = \u001b[38;5;28mopen\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./training_logs/rgb_training_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:527\u001b[39m, in \u001b[36mDataLoader.__len__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m    511\u001b[39m         \u001b[38;5;66;03m# NOTE [ IterableDataset and __len__ ]\u001b[39;00m\n\u001b[32m    512\u001b[39m         \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    525\u001b[39m \n\u001b[32m    526\u001b[39m         \u001b[38;5;66;03m# Cannot statically verify that dataset is Sized\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m         length = \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called = \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[assignment, arg-type]\u001b[39;00m\n\u001b[32m    528\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    529\u001b[39m             \u001b[38;5;28mself\u001b[39m.batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    530\u001b[39m         ):  \u001b[38;5;66;03m# IterableDataset doesn't allow custom sampler or batch_sampler\u001b[39;00m\n\u001b[32m    531\u001b[39m             \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ceil\n",
      "\u001b[31mTypeError\u001b[39m: object of type 'SymbolCropStreamer' has no len()"
     ]
    }
   ],
   "source": [
    "dataloader1 = createDataloader()\n",
    "model1 = MyLeNet()\n",
    "\n",
    "train(model1, dataloader1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29710733",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model1, dataloader1, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
